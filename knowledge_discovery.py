# -*- coding: utf-8 -*-
"""Knowledge Discovery.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11EY4sBEUUOLiX3dH3OY8d6ha2mwuAmBH

Install sklearn or gspread if there are errors
"""

#%pip install gspread
#!pip install --upgrade scikit-learn
#!pip install nltk gensim

import pandas as pd

# import csv
df = pd.read_csv('KnowledgeEconDataset.csv')
df.columns = df.iloc[0]
df = df.iloc[1:]

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Vectorize the abstracts
vectorizer = CountVectorizer(stop_words='english')
data_vectorized = vectorizer.fit_transform(df['Keywords'])

# Apply LDA
lda_model = LatentDirichletAllocation(n_components=10)
lda_model.fit_transform(data_vectorized)

# inspect the topics found by the LDA model
for idx, topic in enumerate(lda_model.components_):
    print(f"Topic {idx + 1}:")
    print("----------")
    for i in topic.argsort()[:-10 - 1:-1]:
        print(f"{vectorizer.get_feature_names_out()[i]}: {topic[i]}")
    print("\n")

import matplotlib.pyplot as plt

def plot_top_words(lda, feature_names, n_top_words):
    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)
    axes = axes.flatten()
    for topic_idx, topic in enumerate(lda.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = [feature_names[i] for i in top_features_ind]
        weights = topic[top_features_ind]

        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.7)
        ax.set_title(f'Topic {topic_idx +1}',
                     fontdict={'fontsize': 30})
        ax.invert_yaxis()
        ax.tick_params(axis='both', which='major', labelsize=20)
        for i in 'top right left'.split():
            ax.spines[i].set_visible(False)
        fig.suptitle('Top words per topic', fontsize=40)

    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
    plt.show()

n_top_words = 20
plot_top_words(lda_model, vectorizer.get_feature_names_out(), n_top_words)

import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from gensim import corpora, models

# Download NLTK's stopword corpus
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Initialize a PorterStemmer for stemming words
stemmer = PorterStemmer()

# Get English stopwords
stop_words = set(stopwords.words('english'))

# Tokenize, remove stop words, and stem
df['processed'] = df['Keywords'].apply(lambda x: [stemmer.stem(w) for w in nltk.word_tokenize(x.lower()) if w not in stop_words])

# Create a dictionary representation of the documents
dictionary = corpora.Dictionary(df['processed'])

# Convert the collection of texts to a bag-of-words corpus
corpus = [dictionary.doc2bow(text) for text in df['processed']]

# Train an LDA model
lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary)

# Print the topics discovered by the LDA model
for idx in range(lda_model.num_topics):
    print(f"Topic {idx + 1}:")
    print("----------")
    for word, prob in lda_model.show_topic(idx):
        print(f"{word}: {prob}")
    print("\n")

from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

# Convert lists of words back to strings
df['processed_str'] = df['processed'].apply(' '.join)

# Create a TF-IDF representation of the data
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df['processed_str'])

kmeans = KMeans(n_clusters=3, n_init=10)
kmeans.fit(X)

# Reduce the dimensionality of your data to 2 components
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(X.toarray())

# Fit KMeans to the reduced data
kmeans = KMeans(n_clusters=3, n_init=10)
kmeans.fit(reduced_data)

# Plot the clustered data
plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans.labels_, cmap='viridis', label='Documents')

# Plot the cluster centers
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', label='Centroids')

# Add labels and a legend
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()

plt.show()

import seaborn as sns

# Set a seed
random_seed = 32

# Select a subset of the data
df_subset = df[df['RESEARCHER'].isin(df['RESEARCHER'].sample(n=10, random_state=random_seed))]
plt.figure(figsize=(15, 10))

# Create a scatterplot
sns.scatterplot(x='RESEARCHER', y='processed_str', data=df_subset, palette='viridis')

# Rotate x-labels for better readability
plt.xticks(rotation=90)

# Increase the size of the labels
plt.tick_params(axis='both', which='major', labelsize=10)
plt.tight_layout()

# Show the plot
plt.show()